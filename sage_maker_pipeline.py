# -*- coding: utf-8 -*-
"""Untitled88.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjbIEn8JGmkpacXuc0CJ7n9KSyBWPjN0
"""

import sys
import boto3
import sagemaker
from sagemaker.workflow.pipeline_context import PipelineSession

# Initialize SageMaker session
sagemaker_session = sagemaker.session.Session()

# Get the AWS region from the current session
region = sagemaker_session.boto_region_name

# Get the execution role associated with the current SageMaker environment
role = sagemaker.get_execution_role()

# Create a pipeline-specific session for SageMaker Pipelines
pipeline_session = PipelineSession()

# Get or create the default S3 bucket associated with the current session
default_bucket = sagemaker_session.default_bucket()

# Define the name for the Model Package Group (used for model registry)
model_package_group_name = "ModelPackageGroupName"

!mkdir -p data #making directore on the container

import boto3
import sagemaker

# Define local path for the batch dataset
local_path = "data/abalone-dataset-batch"

# Download the batch dataset from a public SageMaker Service Catalog bucket
s3 = boto3.resource("s3")
s3.Bucket(f"sagemaker-servicecatalog-seedcode-{region}").download_file(
    "dataset/abalone-dataset-batch", local_path
)

# Define the base S3 path for uploading the batch data
base_uri = f"s3://{default_bucket}/abalone"

# Upload the batch dataset to your SageMaker default bucket
batch_data_uri = sagemaker.s3.S3Uploader.upload(
    local_path=local_path,
    desired_s3_uri=base_uri,
)

# Print the S3 URI for use in Batch Transform step
print(f"Batch data uploaded to: {batch_data_uri}")

#Defining Parameters
from sagemaker.workflow.parameters import (
    ParameterInteger,
    ParameterString,
    ParameterFloat,
)

# ðŸ”§ Define pipeline parameters for flexibility and reusability

# Number of instances used for processing steps (e.g., feature engineering)
processing_instance_count = ParameterInteger(
    name="ProcessingInstanceCount",
    default_value=1
)

# Type of instance to be used in training (e.g., ml.m5.xlarge)
instance_type = ParameterString(
    name="TrainingInstanceType",
    default_value="ml.m5.xlarge"
)

# Approval status for model registration in Model Registry
model_approval_status = ParameterString(
    name="ModelApprovalStatus",
    default_value="PendingManualApproval"
)

# Input data S3 URI (used in processing and training steps)
input_data = ParameterString(
    name="InputData",
    default_value=input_data_uri  # Should be defined from S3Uploader upload
)

# Batch data S3 URI (used in Batch Transform step)
batch_data = ParameterString(
    name="BatchData",
    default_value=batch_data_uri  # Should be defined from S3Uploader upload
)

# MSE threshold for model evaluation (used to determine if model should be registered)
mse_threshold = ParameterFloat(
    name="MseThreshold",
    default_value=6.0
)

!mkdir -p code # Make direcotory for the code

# Commented out IPython magic to ensure Python compatibility.
# %%writefile code/preprocessing.py
# """
# Preprocessing Script for Abalone Dataset (SageMaker Compatible)
# 
# This script performs data preprocessing for the Abalone dataset,
# including imputation, scaling, one-hot encoding, and train/validation/test splitting.
# It is designed to run within an Amazon SageMaker Processing Job.
# 
# Output files:
#     - /opt/ml/processing/train/train.csv
#     - /opt/ml/processing/validation/validation.csv
#     - /opt/ml/processing/test/test.csv
# """
# 
# import os
# import numpy as np
# import pandas as pd
# 
# from sklearn.compose import ColumnTransformer
# from sklearn.impute import SimpleImputer
# from sklearn.pipeline import Pipeline
# from sklearn.preprocessing import StandardScaler, OneHotEncoder
# 
# # Column names for features and label
# feature_columns_names = [
#     "sex",
#     "length",
#     "diameter",
#     "height",
#     "whole_weight",
#     "shucked_weight",
#     "viscera_weight",
#     "shell_weight",
# ]
# label_column = "rings"
# 
# # Data types for input columns
# feature_columns_dtype = {
#     "sex": str,
#     "length": np.float64,
#     "diameter": np.float64,
#     "height": np.float64,
#     "whole_weight": np.float64,
#     "shucked_weight": np.float64,
#     "viscera_weight": np.float64,
#     "shell_weight": np.float64,
# }
# label_column_dtype = {"rings": np.float64}
# 
# 
# def merge_two_dicts(x, y):
#     """
#     Merge two dictionaries into a single dictionary.
#     Useful for combining feature and label dtype definitions.
#     """
#     z = x.copy()
#     z.update(y)
#     return z
# 
# 
# if __name__ == "__main__":
#     # SageMaker-specific base directory for input/output
#     base_dir = "/opt/ml/processing"
# 
#     # Load the raw CSV dataset from input directory with no headers
#     df = pd.read_csv(
#         f"{base_dir}/input/abalone-dataset.csv",
#         header=None,
#         names=feature_columns_names + [label_column],
#         dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),
#     )
# 
#     # Define numeric feature processing pipeline: median imputation + standard scaling
#     numeric_features = list(feature_columns_names)
#     numeric_features.remove("sex")  # Categorical feature
#     numeric_transformer = Pipeline(
#         steps=[
#             ("imputer", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler())
#         ]
#     )
# 
#     # Define categorical feature processing pipeline: constant imputation + one-hot encoding
#     categorical_features = ["sex"]
#     categorical_transformer = Pipeline(
#         steps=[
#             ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
#             ("onehot", OneHotEncoder(handle_unknown="ignore"))
#         ]
#     )
# 
#     # Combine numerical and categorical pipelines
#     preprocess = ColumnTransformer(
#         transformers=[
#             ("num", numeric_transformer, numeric_features),
#             ("cat", categorical_transformer, categorical_features)
#         ]
#     )
# 
#     # Separate target variable from features
#     y = df.pop(label_column)
# 
#     # Apply preprocessing to features
#     X_pre = preprocess.fit_transform(df)
#     y_pre = y.to_numpy().reshape(len(y), 1)
# 
#     # Combine preprocessed features and labels
#     X = np.concatenate((y_pre, X_pre), axis=1)
# 
#     # Shuffle and split the dataset into train (70%), validation (15%), and test (15%)
#     np.random.shuffle(X)
#     train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])
# 
#     # Save processed datasets to SageMaker-compatible output folders
#     pd.DataFrame(train).to_csv(f"{base_dir}/train/train.csv", header=False, index=False)
#     pd.DataFrame(validation).to_csv(f"{base_dir}/validation/validation.csv", header=False, index=False)
#     pd.DataFrame(test).to_csv(f"{base_dir}/test/test.csv", header=False, index=False)

from sagemaker.sklearn.processing import SKLearnProcessor

# Define the Scikit-learn framework version to be used in the processing container
framework_version = "1.2-1"

# Create an SKLearnProcessor object to handle the preprocessing script
sklearn_processor = SKLearnProcessor(
    framework_version=framework_version,       # Version of the scikit-learn container
    instance_type="ml.m5.xlarge",              # Type of instance for processing
    instance_count=processing_instance_count,  # Number of instances (parameterized)
    base_job_name="sklearn-abalone-process",   # Prefix for the SageMaker job name
    role=role,                                 # IAM role with SageMaker permissions
    sagemaker_session=pipeline_session         # Pipeline-specific session for SageMaker Pipelines
)

from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.workflow.steps import ProcessingStep

# Define arguments for the preprocessing job using the SKLearnProcessor
processor_args = sklearn_processor.run(
    # Input location (raw dataset in S3) â†’ mapped to /opt/ml/processing/input
    inputs=[
        ProcessingInput(
            source=input_data,  # ParameterString passed from pipeline
            destination="/opt/ml/processing/input"
        ),
    ],

    # Output locations for processed datasets (train, validation, test)
    outputs=[
        ProcessingOutput(
            output_name="train",
            source="/opt/ml/processing/train"
        ),
        ProcessingOutput(
            output_name="validation",
            source="/opt/ml/processing/validation"
        ),
        ProcessingOutput(
            output_name="test",
            source="/opt/ml/processing/test"
        ),
    ],

    # Path to the preprocessing script
    code="code/preprocessing.py",
)

# Define the processing step in the SageMaker pipeline
step_process = ProcessingStep(
    name="AbaloneProcess",        # Name of the step in the pipeline
    step_args=processor_args      # Processing job configuration from above
)

from sagemaker.estimator import Estimator
from sagemaker.inputs import TrainingInput

# Define the S3 location for saving trained model artifacts
model_path = f"s3://{default_bucket}/AbaloneTrain"

# Retrieve the pre-built XGBoost container URI for the specified region and version
image_uri = sagemaker.image_uris.retrieve(
    framework="xgboost",
    region=region,
    version="1.0-1",        # XGBoost version
    py_version="py3",       # Python 3
    instance_type="ml.m5.xlarge"  # Helps select the correct container variant
)

# Define the XGBoost Estimator
xgb_train = Estimator(
    image_uri=image_uri,
    instance_type=instance_type,  # ParameterString defined earlier
    instance_count=1,
    output_path=model_path,       # S3 output path for model artifacts
    role=role,                    # IAM role with SageMaker access
    sagemaker_session=pipeline_session  # Use pipeline-aware session
)

# Set hyperparameters for training
xgb_train.set_hyperparameters(
    objective="reg:linear",       # Regression task
    num_round=50,                 # Number of boosting rounds
    max_depth=5,
    eta=0.2,
    gamma=4,
    min_child_weight=6,
    subsample=0.7,
)

# Define training input sources using the outputs of the preprocessing step
train_args = xgb_train.fit(
    inputs={
        "train": TrainingInput(
            s3_data=step_process.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri,
            content_type="text/csv"
        ),
        "validation": TrainingInput(
            s3_data=step_process.properties.ProcessingOutputConfig.Outputs["validation"].S3Output.S3Uri,
            content_type="text/csv"
        )
    }
)

# Wrap the training job in a SageMaker pipeline step
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.steps import TrainingStep
step_train = TrainingStep(
    name="AbaloneTrain",     # Pipeline step name
    step_args=train_args     # Args returned from estimator.fit(...)
)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile code/evaluation.py
# """
# Evaluation Script for SageMaker Model Processing Step
# 
# This script:
# - Loads the trained XGBoost model from model.tar.gz
# - Loads the test dataset from the specified input location
# - Runs predictions on the test set
# - Computes the Mean Squared Error (MSE) and standard deviation
# - Writes evaluation metrics to /opt/ml/processing/evaluation/evaluation.json
# 
# This format is compatible with SageMaker Model Evaluation and conditional logic.
# """
# 
# import json
# import pathlib
# import pickle
# import tarfile
# 
# import joblib
# import numpy as np
# import pandas as pd
# import xgboost
# from sklearn.metrics import mean_squared_error
# 
# if __name__ == "__main__":
#     # Unpack the trained model from tar.gz archive
#     model_path = "/opt/ml/processing/model/model.tar.gz"
#     with tarfile.open(model_path) as tar:
#         tar.extractall(path=".")  # Extracts to current working directory
# 
#     # Load the model file (must match the name used in training output)
#     model = pickle.load(open("xgboost-model", "rb"))
# 
#     # Load the test data (1st column is the label)
#     test_path = "/opt/ml/processing/test/test.csv"
#     df = pd.read_csv(test_path, header=None)
# 
#     # Separate features and labels
#     y_test = df.iloc[:, 0].to_numpy()           # True labels
#     df.drop(df.columns[0], axis=1, inplace=True)  # Drop label column
#     X_test = xgboost.DMatrix(df.values)         # XGBoost DMatrix for prediction
# 
#     # Predict and evaluate
#     predictions = model.predict(X_test)
#     mse = mean_squared_error(y_test, predictions)
#     std = np.std(y_test - predictions)
# 
#     # Prepare JSON report for SageMaker model evaluation
#     report_dict = {
#         "regression_metrics": {
#             "mse": {"value": mse, "standard_deviation": std},
#         }
#     }
# 
#     # Save the evaluation report
#     output_dir = "/opt/ml/processing/evaluation"
#     pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)
# 
#     evaluation_path = f"{output_dir}/evaluation.json"
#     with open(evaluation_path, "w") as f:
#         json.dump(report_dict, f)

from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput

# Define a ScriptProcessor to run evaluation using the same image as training (XGBoost)
script_eval = ScriptProcessor(
    image_uri=image_uri,                    # XGBoost container image used earlier
    command=["python3"],                    # Command to execute the script
    instance_type="ml.m5.xlarge",           # Instance type for evaluation
    instance_count=1,                       # Number of instances (can be 1 for eval)
    base_job_name="script-abalone-eval",    # Prefix for job name
    role=role,                              # IAM role for execution
    sagemaker_session=pipeline_session      # Session used within the pipeline
)

# Define inputs and outputs for evaluation script
eval_args = script_eval.run(
    inputs=[
        # Pass trained model artifacts from the training step
        ProcessingInput(
            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,
            destination="/opt/ml/processing/model"
        ),
        # Pass test dataset from preprocessing step
        ProcessingInput(
            source=step_process.properties.ProcessingOutputConfig.Outputs["test"].S3Output.S3Uri,
            destination="/opt/ml/processing/test"
        ),
    ],
    outputs=[
        # Output path for evaluation.json (used by ConditionStep)
        ProcessingOutput(
            output_name="evaluation",
            source="/opt/ml/processing/evaluation"
        ),
    ],
    code="code/evaluation.py"  # Path to your evaluation script
)

from sagemaker.workflow.properties import PropertyFile
from sagemaker.workflow.steps import ProcessingStep

# Define a PropertyFile to reference the evaluation.json output
evaluation_report = PropertyFile(
    name="EvaluationReport",              # Name used to refer to this property
    output_name="evaluation",             # Matches the output_name in ProcessingOutput
    path="evaluation.json"                # Relative path inside /opt/ml/processing/evaluation
)

# Wrap the evaluation job as a ProcessingStep in the pipeline
step_eval = ProcessingStep(
    name="AbaloneEval",                   # Pipeline step name
    step_args=eval_args,                  # Arguments returned from ScriptProcessor.run(...)
    property_files=[evaluation_report]    # Attach property file to extract metrics
)

from sagemaker.model import Model

# Define a SageMaker Model object to use for registration or deployment
model = Model(
    image_uri=image_uri,  # Same image used for training (e.g., XGBoost)
    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,  # S3 URI from training step
    sagemaker_session=pipeline_session,  # Pipeline-aware session
    role=role,  # IAM role with SageMaker model registration permissions
)

from sagemaker.workflow.model_step import ModelStep

# Define a model creation step using the previously defined SageMaker Model
step_create_model = ModelStep(
    name="AbaloneCreateModel",  # Name of the pipeline step
    step_args=model.create(
        instance_type="ml.m5.large",             # Target instance type for hosting
        accelerator_type="ml.eia1.medium"        # Optional: attach Elastic Inference accelerator
    ),
)

from sagemaker.transformer import Transformer

# Define a Transformer object for batch inference using the created model
transformer = Transformer(
    model_name=step_create_model.properties.ModelName,  # Reference model created in pipeline
    instance_type="ml.m5.xlarge",                       # Instance type for batch transform
    instance_count=1,                                   # Number of instances for the job
    output_path=f"s3://{default_bucket}/AbaloneTransform",  # S3 location to save predictions
)

from sagemaker.inputs import TransformInput
from sagemaker.workflow.steps import TransformStep

# Define the TransformStep for batch scoring in the SageMaker Pipeline
step_transform = TransformStep(
    name="AbaloneTransform",  # Name of the pipeline step
    transformer=transformer,  # Transformer object defined earlier
    inputs=TransformInput(
        data=batch_data,       # S3 path to batch input data (passed as a pipeline parameter)
        content_type="text/csv",  # Format of input data
        split_type="Line"         # Delimiter for batch records
    )
)